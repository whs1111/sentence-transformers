{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whs1111/sentence-transformers/blob/master/src/CLASS/trec4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMb0U7ctSDcL",
        "colab_type": "code",
        "outputId": "7ac885d2-6ec7-4e67-99b6-95bf89bdbd4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dBPVVL26Lyz",
        "colab_type": "code",
        "outputId": "e218ee14-483e-40c3-c190-4469b14b7ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My Drive/Colab_Notebooks/code/bert-version/src/attention"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab_Notebooks/code/bert-version/src/attention\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdZjkab-PZ7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LabelTable = {}\n",
        "LabelTable[\"GoodsServices\"] = [\"Request-GoodsServices\", \"Request\", \"The user is asking for a particular service or physical good.\", 0]\n",
        "LabelTable[\"SearchAndRescue\"] = [\"Request-SearchAndRescue\", \"Request\", \"The user is requesting a rescue (for themselves or others)\", 1]\n",
        "LabelTable[\"InformationWanted\"] = [\"Request-InformationWanted\", \"Request\", \"The user is requesting informationd\", 2]\n",
        "LabelTable[\"Volunteer\"] = [\"CallToAction-Volunteer\", \"Call to action\", \"The user is asking people to volunteer to help the response effort\", 3]\n",
        "LabelTable[\"Donations\"] = [\"CallToAction-Donations\", \"Call to action\", \"The user is asking people to donate goods/money\", 4]\n",
        "LabelTable[\"MovePeople\"] = [\"CallToAction-MovePeople\", \"Call to action\", \"The user is asking people to leave an area or go to another area\", 5]\n",
        "LabelTable[\"FirstPartyObservation\"] = [\"Report-FirstPartyObservation\", \"report\", \"The user is giving an eye-witness account\", 6]\n",
        "LabelTable[\"ThirdPartyObservation\"] = [\"Report-ThirdPartyObservation\", \"report\", \"The user is reporting a information that they received from someone else\", 7]\n",
        "LabelTable[\"Weather\"] = [\"Report-Weather\", \"report\", \"The user is providing a weather report (current or forecast)\", 8]\n",
        "LabelTable[\"EmergingThreats\"] = [\"Report-EmergingThreats\", \"report\", \"The user is reporting a potential problem that may cause future loss of life or damage\", 9]\n",
        "LabelTable[\"SignificantEventChange\"] = [\"Report-SignificantEventChange\", \"report\", \"The user is reporting a new occuirrence that public safety officers need to respond to.\", 10]\n",
        "LabelTable[\"MultimediaShare\"] = [\"Report-MultimediaShare\", \"report\", \"The user is sharing images or video\", 11]\n",
        "LabelTable[\"ServiceAvailable\"] = [\"Report-ServiceAvailable\", \"report\", \"The user is reporting that they or someone else is providing a service\", 12]\n",
        "LabelTable[\"Factoid\"] = [\"Report-Factoid\", \"report\", \"The user is relating some facts, typically numerical\", 13]\n",
        "LabelTable[\"Official\"] = [\"Report-Official\", \"report\", \"An official report by a government or public safety representative\", 14]\n",
        "LabelTable[\"CleanUp\"] = [\"Report-CleanUp\", \"report\", \"A report of the clean up after the event\", 15]\n",
        "LabelTable[\"Hashtags\"] = [\"Report-Hashtags\", \"report\", \"Reporting which hashtags correspond to each event\", 16]\n",
        "LabelTable[\"PastNews\"] = [\"Other-PastNews\", \"other\", \"The post is generic news, e.g. reporting that the event occurred\", 17]\n",
        "LabelTable[\"ContinuingNews\"] = [\"Other-ContinuingNews\", \"other\", \"The post providing/linking to continious coverage of the event\", 18]\n",
        "LabelTable[\"Advice\"] = [\"Other-Advice\", \"other\", \"The  author is providing some advice to the public\", 19]\n",
        "LabelTable[\"Sentiment\"] = [\"Other-Sentiment\", \"other\", \"The post is expressing some sentiment about the event\", 20]\n",
        "LabelTable[\"Discussion\"] = [\"Other-Discussion\", \"other\", \"Users are discussing the event\", 21]\n",
        "LabelTable[\"Irrelevant\"] = [\"Other-Irrelevant\", \"other\", \"The post is irrelevant, contains no information\", 22]\n",
        "LabelTable[\"Unknown\"] = [\"Other-Unknown\", \"other\", \"Does not fit into any other category\", 23]\n",
        "LabelTable[\"KnownAlready\"] = [\"Other-KnownAlready\", \"other\", \"The Responder already knows this information\", 24]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUEyn78YBs5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4XFda_IIpeM",
        "colab_type": "code",
        "outputId": "8e20a6ac-81cf-4d05-d5dd-e11a8fddb1cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "!pip  install -U sentence-transformers"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/32/e3d405806ea525fd74c2c79164c3f7bc0b0b9811f27990484c6d6874c76f/sentence-transformers-0.2.5.1.tar.gz (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.9MB/s \n",
            "\u001b[?25hCollecting transformers==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 18.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (1.11.15)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0->sentence-transformers) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 39.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 25.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (1.14.15)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0->sentence-transformers) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0->sentence-transformers) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0->sentence-transformers) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.3.0->sentence-transformers) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.3.0->sentence-transformers) (0.15.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.5.1-cp36-none-any.whl size=67076 sha256=2fa93aa11018c840b412f658ab9a3b3affc00f7a99f85050e2a7aafb21f0d61c\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/ca/b4/7ca542b411730a8840f8e090df2ddacffa1c4dd9f209684c19\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=cb3ca7e0df00b8913c3c26ed8ae3161c511b8c984ffbc2c9f2fdf83c71e300e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.38 sentence-transformers-0.2.5.1 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwCPXJLxd27_",
        "colab_type": "code",
        "outputId": "6831ca4e-849c-457e-9520-d25529c00400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:06<00:00, 60.3MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l6kCm5gbsGN",
        "colab_type": "code",
        "outputId": "439bf6c2-1f01-468b-f4e5-1c2514645a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "text = np.load(\"./data/text.npy\",allow_pickle=True)\n",
        "print(type(text[1]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLprvVUK458L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "label_list = [\"GoodsServices\",\n",
        "        \"SearchAndRescue\",\n",
        "        \"InformationWanted\",\n",
        "        \"Volunteer\",\n",
        "        \"FundRaising\",\n",
        "        \"Donations\",\n",
        "        \"MovePeople\",\n",
        "        \"FirstPartyObservation\",\n",
        "        \"ThirdPartyObservation\",\n",
        "        \"Weather\",\n",
        "        \"EmergingThreats\",\n",
        "        \"NewSubEvent\",\n",
        "        \"MultimediaShare\",\n",
        "        \"ServiceAvailable\",\n",
        "        \"Factoid\",\n",
        "        \"Official\",\n",
        "        \"CleanUp\",\n",
        "        \"Hashtags\",\n",
        "        \"ContextualInformation\",\n",
        "        \"News\",\n",
        "        \"Advice\",\n",
        "        \"Sentiment\",\n",
        "        \"Discussion\",\n",
        "        \"Irrelevant\",\n",
        "        \"OriginalEvent\"]\n",
        "important_list = [\"Low\",\n",
        "        \"Medium\",\n",
        "        \"High\",\n",
        "        \"Critical\"\n",
        "       ]\n",
        "tweets = []\n",
        "path = '/content/gdrive/My Drive/Colab_Notebooks/code/bert-version/src/DATA/ll.csv'\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for line in reader:\n",
        "        tweet_full = line\n",
        "        tweets.append({\n",
        "            'id': tweet_full[0],\n",
        "            'label':tweet_full[1],\n",
        "            'important':tweet_full[2],\n",
        "            'text': tweet_full[3].lower(),\n",
        "            # 'name': tweet_full['user']['name'].split()[0]\n",
        "            })\n",
        "tweet_list = []\n",
        "text_list = []\n",
        "label_list = []\n",
        "for i in range(len(tweets)):\n",
        "        sentences = []\n",
        "        text_list.append(tweets[i]['text'])\n",
        "        tweets_vector = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "        tweets_label = eval(tweets[i]['label'])\n",
        "        p = 0\n",
        "        for j in label_list:\n",
        "            for k in tweets_label:\n",
        "                if k == j:\n",
        "                    tweets_vector[p] = 1\n",
        "                    sentences.append(LabelTable[j][2])\n",
        "            p+=1\n",
        "        tweet_list.append(tweets_vector)\n",
        "        sentence_embeddings = model.encode(sentences)\n",
        "        temple = np.zeros(768)\n",
        "        for embedding in  sentence_embeddings:\n",
        "             temple+=embedding\n",
        "        label_list.append(temple)\n",
        "text_numpy = model.encode(text_list)\n",
        "numpy_label_list = np.array(tweet_list)\n",
        "numpy_text_list = np.array(text_numpy)\n",
        "numpy_label_text_list = np.array(label_list)\n",
        "np.save('./data/label_text.npy',numpy_label_text_list)\n",
        "np.save('./data/label1.npy',numpy_label_list)\n",
        "np.save('./data/text1.npy',numpy_text_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cB8lQscg6JS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "lr = 0.001\n",
        "epochs = 1000\n",
        "batch_size = 16\n",
        "seed = 1111\n",
        "cuda_able = True\n",
        "save = \n",
        "dropout = 0.5\n",
        "embed_dim = 64\n",
        "hidden_size = 32\n",
        "bidirectional = True\n",
        "weight_decay = 0.001\n",
        "attention_size = 16\n",
        "sequence_length = 16\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "use_cuda = torch.cuda.is_available() and cuda_able\n",
        "\n",
        "\n",
        "###################################################\n",
        "#load data\n",
        "\n",
        "from data_loder import DataLoader\n",
        "\n",
        "data = np.load(\"./data/text.npy\")\n",
        "label = np.load(\"./data/label.npy\")\n",
        "\n",
        "\n",
        "\n",
        "training_data = DataLoader(data,\n",
        "                           label,\n",
        "                           batch_size=batch_size,\n",
        "                           cuda=use_cuda)\n",
        "validation_data = DataLoader(data,\n",
        "                             label,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=False,\n",
        "                             cuda=use_cuda)\n",
        "\n",
        "\n",
        "###############################################\n",
        "#build model\n",
        "\n",
        "import model\n",
        "lstm_attn = model.bilstm_attn(batch_size=batch_size,\n",
        "                                  output_size=output_size,\n",
        "                                  hidden_size=hidden_size,\n",
        "                                  vocab_size=vocab_size,\n",
        "                                  embed_dim=embed_dim,\n",
        "                                  bidirectional=bidirectional,\n",
        "                                  dropout=dropout,\n",
        "                                  use_cuda=use_cuda,\n",
        "                                  attention_size=attention_size,\n",
        "                                  sequence_length=sequence_length)\n",
        "if use_cuda:\n",
        "    lstm_attn = lstm_attn.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(lstm_attn.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "###################################################\n",
        "#training\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "accuracy = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate():\n",
        "    lstm_attn.eval()\n",
        "    corrects = eval_loss = 0\n",
        "    _size = validation_data.sents_size\n",
        "\n",
        "    for data, label in tqdm(validation_data, mininterval=0.2,\n",
        "                desc='Evaluate Processing', leave=False):\n",
        "\n",
        "        pred = lstm_attn(data)\n",
        "        loss = criterion(pred, label)\n",
        "\n",
        "        eval_loss += loss.data\n",
        "        corrects += (torch.max(pred, 1)[1].view(label.size()).data == label.data).sum()\n",
        "    return eval_loss[0]/_size, corrects, corrects*100.0/_size, _size\n",
        "\n",
        "\n",
        "def train():\n",
        "    lstm_attn.train()\n",
        "    total_loss = 0\n",
        "    for data, label in tqdm(training_data, mininterval=1,\n",
        "                desc='Train Processing', leave=False):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        target = lstm_attn(data)\n",
        "        loss = criterion(target, label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.data\n",
        "    return total_loss[0]/training_data.sents_size\n",
        "\n",
        "#################################################\n",
        "#saving\n",
        "best_acc = None\n",
        "total_start_time = time.time()\n",
        "\n",
        "try:\n",
        "    print('-' * 90)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        loss = train()\n",
        "        train_loss.append(loss*1000.)\n",
        "\n",
        "        print('| start of epoch {:3d} | time: {:2.2f}s | loss {:5.6f}'.format(epoch,\n",
        "                                                                              time.time() - epoch_start_time,\n",
        "                                                                              loss))\n",
        "\n",
        "        loss, corrects, acc, size = evaluate()\n",
        "        valid_loss.append(loss*1000.)\n",
        "        accuracy.append(acc)\n",
        "\n",
        "        print('-' * 10)\n",
        "        print('| end of epoch {:3d} | time: {:2.2f}s | loss {:.4f} | accuracy {}%({}/{})'.format(epoch,\n",
        "                                                                                                 time.time() - epoch_start_time,\n",
        "                                                                                                 loss,\n",
        "                                                                                                 acc,\n",
        "                                                                                                 corrects,\n",
        "                                                                                                 size))\n",
        "        print('-' * 10)\n",
        "        if not best_acc or best_acc < corrects:\n",
        "            best_acc = corrects\n",
        "            model_state_dict = lstm_attn.state_dict()\n",
        "            model_source = {\n",
        "                \"model\": model_state_dict,\n",
        "                \"src_dict\": data['dict']['train']\n",
        "            }\n",
        "            torch.save(model_source, save)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"-\"*90)\n",
        "    print(\"Exiting from training early | cost time: {:5.2f}min\".format((time.time() - total_start_time)/60.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gWoWziH5-c3",
        "colab_type": "text"
      },
      "source": [
        "# 新段落"
      ]
    }
  ]
}